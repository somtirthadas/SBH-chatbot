# -*- coding: utf-8 -*-
"""test_llama_ft1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p-rdP6efldyY9z5uUiQ-TarNfV2ZzEDD

INSTALL REQUIRED MODELS
"""

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

"""IMPORT MODULES"""

import os
import torch
#from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    #HfArgumentParser,
    #TrainingArguments,
    pipeline,
    logging,
)
#from peft import LoraConfig, PeftModel
#from trl import SFTTrainer

"""GPU CONFIGURATION"""

import accelerate
device_map = {"": 0}

"""FIX MODEL CONFIGURATIONS"""

model_name = "somtirthadas2004/Llama-2-7b-chat-finetune2"

use_4bit = True                         #Activate 4 bit precision
bnb_4bit_compute_dtype = "float16"      #Compute data type
bnb_4bit_quant_type = "nf4"             #quantization type (fp4 or nf4)
use_nested_quant = False

compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config, device_map = device_map)
model.config.use_cache = False
model.config.pretraining_tp = 1

stop_words=("exit", "good Bye", "not required", "no", "no thanks", "thank you", "Thank you", "EXIT", "Exit", "No Thanks", "Not Needed", "No", "Bye")

logging.set_verbosity(logging.CRITICAL)

pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, temperature=0.6, top_p=0.9, max_length=300)

flag1=0
flag2=0

print("Hi welcome to Career Mitra, your own career help chat-bot.")
while True:
  print("To start with us, tell me your current working field or area of interest.")
  prompt1=input()
  print("Good, now in that context what are the skills that you have?")
  prompt2=input()
  result = pipe(f"<s>[INST] With this area of interest {prompt1} and skillset {prompt2} what can be my suitable career? [/INST]")
  print(result[0]['generated_text'])
  while True:
    print("Any further queries or clarification you need regarding my response?")
    prompt=input()
    for words in stop_words:
      if prompt == words:
        print("Thank you, Have a nice day.")
        flag1 = 1
        flag2 = 1
        break
    if flag2 == 1:
      break
    result = pipe(f"<s>[INST] {prompt} [/INST]")
    print(result[0]['generated_text'])
  if flag1 == 1:
    break

"""END"""